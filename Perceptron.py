#Import libraries
################################################
import matplotlib.pyplot as plt
import numpy as np

# Various concentrations of sodium ions (Na+) in the synapse
# V action or no action
# dataset self generated
# -70mV (resting potential) to -55mV or -50mV (action potential) of Na+
# Reference Principles of Neurobiology (see pages 48 - 50)

# A simple experiment was devised to find out what voltage is required to
# cause a neuron to fire. Two electrodes are connected to both ends of
# a squid giant axon. The giant axon's large diameter and rapid conductance of 
# an action potential (neuron quick to fire) make it a suitable choice for testing [See reference]
# Figure 1.1 shows a diagram of the set-up. The electrode on the left in figure 1.1
# is a stimulating electrode and the electrode on the right is a recording
# electrode. The stimulating electrode injects step current pulses with varying
# magnitudes at random. The voltage injected and activation of the neuron are both
# recorded and summarized in the table.

# We can use a single perceptron to find the boundary between activation and no activation (i.e. the voltage
# required to cause the neuron to fire).

# The following code concatenates two datasets (neuron firing and not firing)
# generated by uniform distributions (midpoint = action potential)
# and fits a perceptron to the data

#Generate dataset
################################################
np.random.seed(1) # Set seed for reproducing
mean = -55        # Set mean/ boundary point
n = 10            # Set number of data points

overlap = np.random.uniform(low=0,high=5) # Set amount of overlap
# Create action potential dataset
action_potential_data = np.transpose(np.array([np.ones(n//2), 
                                               np.random.uniform(low=mean-overlap, 
                                                                 high=-40, 
                                                                 size=5)]))
# Create resting potential dataset
resting_potential_data = np.transpose(np.array([-np.ones(n//2),
                                                np.random.uniform(low=-70, 
                                                                  high=mean+overlap, 
                                                                  size=5)]))
# Combine datasets together
neuron_data = np.concatenate((action_potential_data, resting_potential_data),axis=0)
# Randomly permute datasets
neuron_data = np.random.permutation(neuron_data)

# Split dataset into observations and responses
################################################
X = neuron_data[:,1]
Y = neuron_data[:, 0]    

# Define functions and loss functions 
################################################
def perceptron(x):
    return np.heaviside(x,0)
def linear(x,w,b):
    return w*x+b

def loss(y, x, w, b):
  return -y*(w*x+b)
def dw_loss(x,y):
  return -y*x
def db_loss(y):
  return -y


# Numeric loop; Stochastic Gradient descent
################################################
loss_v = 0
nu=1                             # Set learning rate
w = np.random.uniform(-15,15)   # Initialise w
b = np.random.uniform(-15,15)   # Initialise b
X = neuron_data[:,1]
Y = neuron_data[:, 0]   
correct_X = X
correct_Y = Y
epochs = 800
for iter in range(epochs):          # Number of iterations
  for t in range(len(X)):           # Iterate over data set 
    x = X[t]                        # Get current data point
    y = Y[t]                        # Get current data point's response
    if (2*perceptron(linear(x,w,b))-1 != y):    # Update only if missclassified
      w = w - nu*dw_loss(x,y)                   # Update w
      b = b - nu*db_loss(y)                     # Update b
    loss_v = loss(y, x, w, b)       # calculate loss
print(w,b)
print(2*perceptron(linear(neuron_data[:,1],w,b))-1)


# Plot results
################################################
x = np.linspace(-75, -35, 100)
y = perceptron(linear(x,1,-beta_hat[0]))

plt.style.use('seaborn')
fig, ax = plt.subplots()

# Draw points
ax.scatter(X, (Y+1)/2, color='#71A8CF', label='Neuronal responses')
# Draw points
k =np.linspace(-70,-40,20)
ax.plot(k, perceptron(linear(k,w,b)) , color='#FFAD67', label='f(z)=H(z)')
ax.plot(k, linear(k,w,b) , color='#0015ff', label='Decision boundary')
# Set captions
plt.xlabel('Na+ Concentration (mV)')
plt.ylabel('Action')
plt.title('Na+ Concentration and Action.')
#Set xlim, ylim (plot range)
plt.xlim([-75, -35])
plt.ylim([-0.5, 1.5])
#Show only 0, 1 labels
labels = [item.get_text() for item in ax.get_yticklabels()]
ax.set_yticks([0,1])
ax.set_yticklabels(['Action', 'No Action'])
ax.legend(loc='upper left')
fig.savefig("scatter_perceptron.png")

